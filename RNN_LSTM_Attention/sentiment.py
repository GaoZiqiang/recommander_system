# -*- coding: utf-8 -*-
"""
LSTM项目实战：情感分类问题
Automatically generated by Colaboratory.
Original file is located at
    https://colab.research.google.com/drive/1GX0Rqur8T45MSYhLU9MYWAbycfLH4-Fu
"""

import torch
from torch import nn, optim
from torchtext.legacy import data, datasets

# print('GPU:', torch.cuda.is_available())

torch.manual_seed(123)

TEXT = data.Field(tokenize='spacy')
LABEL = data.LabelField(dtype=torch.float)
train_data, test_data = datasets.IMDB.splits(TEXT, LABEL)

print('len of train data:', len(train_data))# len(train_data) = 25000
print('len of test data:', len(test_data))# len(test_data) = 25000

print(train_data.examples[15].text)
print(train_data.examples[15].label)

# word2vec, glove
# 使用glove做word2vec操作 max_siz调整为100
TEXT.build_vocab(train_data, max_size=100, vectors='glove.6B.100d')
LABEL.build_vocab(train_data)

batchsize = 30
# device = torch.device('cuda')
device = torch.device("cuda" if torch.cuda.is_available() else "cpu")
train_iterator, test_iterator = data.BucketIterator.splits(
    (train_data, test_data),
    batch_size=batchsize,
    device=device
)
print("the len of train_iterator is ",len(train_iterator))

class LSTMNet(nn.Module):

    def __init__(self, vocab_size, embedding_dim, hidden_dim):
        """
        """
        super(LSTMNet, self).__init__()

        # [0-10001] => [100]
        # Embedding生成器
        self.embedding = nn.Embedding(vocab_size, embedding_dim)# vocab_size=10000,embedding_dim=100
        # [100] => [256]
        self.lstm = nn.LSTM(embedding_dim, hidden_dim, num_layers=2,
                            bidirectional=True, dropout=0.5)# embedding_dim=100,hidden_dim=256 dropout防止overfit
        # [256*2] => [1]
        self.fc = nn.Linear(hidden_dim * 2, 1)
        self.dropout = nn.Dropout(0.5)

    def forward(self, x):
        """
        x: [seq_len, b] vs [b, 3, 28, 28]
        """
        # [seq, b, 1] => [seq, b, 100]
        # 将文本数据转换成embedding数据
        embedding = self.dropout(self.embedding(x))

        # output: [seq, b, hid_dim*2]
        # hidden/h: [num_layers*2, b, hid_dim]
        # cell/c: [num_layers*2, b, hid_di]
        # 将input data输入RNN
        output, (hidden, cell) = self.lstm(embedding)

        # [num_layers*2, b, hid_dim] => 2 of [b, hid_dim] => [b, hid_dim*2]
        hidden = torch.cat([hidden[-2], hidden[-1]], dim=1)

        # [b, hid_dim*2] => [b, 1]
        hidden = self.dropout(hidden)
        out = self.fc(hidden)

        return out

# 生成LSTMNet对象
lstm = LSTMNet(len(TEXT.vocab), 100, 256)

pretrained_embedding = TEXT.vocab.vectors
print('pretrained_embedding:', pretrained_embedding.shape)
lstm.embedding.weight.data.copy_(pretrained_embedding)
print('embedding layer inited.')

optimizer = optim.Adam(lstm.parameters(), lr=1e-3)
criteon = nn.BCEWithLogitsLoss().to(device)
lstm.to(device)

import numpy as np


def binary_acc(preds, y):
    """
    get accuracy 计算预测的准确度
    """
    preds = torch.round(torch.sigmoid(preds))
    correct = torch.eq(preds, y).float()
    acc = correct.sum() / len(correct)
    return acc


def train(rnn, iterator, optimizer, criteon):
    avg_acc = []
    rnn.train()

    for i, batch in enumerate(iterator):

        # [seq, b] => [b, 1] => [b]
        # 解释：rnn(batch.text)输出为[b,1]，为了方便与label（其为[b]）计算loss，需要将rnn(batch.text)输出转换为[b]
        pred = rnn(batch.text).squeeze(1)
        #
        loss = criteon(pred, batch.label)
        # 计算准确度/精确度
        acc = binary_acc(pred, batch.label).item()
        avg_acc.append(acc)

        optimizer.zero_grad()
        loss.backward()
        optimizer.step()

        if i % 10 == 0:
            print("迭代次数:" ,i, acc)

    avg_acc = np.array(avg_acc).mean()
    print("本次epoch的avg acc为:",avg_acc)


def eval(rnn, iterator, criteon):
    avg_acc = []

    rnn.eval()

    with torch.no_grad():
        for batch in iterator:
            # [b, 1] => [b]
            pred = rnn(batch.text).squeeze(1)

            #
            loss = criteon(pred, batch.label)

            acc = binary_acc(pred, batch.label).item()
            avg_acc.append(acc)

    avg_acc = np.array(avg_acc).mean()

    print('本次test的avg_acc为:', avg_acc)


for epoch in range(10):
    print("------start training------")
    print("------ epoch:",epoch)
    # eval(rnn, test_iterator, criteon)
    train(lstm, train_iterator, optimizer, criteon)

print("------start testing------")
eval(lstm, test_iterator, criteon)